{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manasvi\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "index = []\n",
    "labels = []\n",
    "keep = ['not', 'no', 'for', 'and', 'nor', 'but', 'or', 'so',\n",
    "        'while', 'if', 'only', \n",
    "        'until', 'than', \n",
    "         'as', 'after', 'before',\n",
    "        'by', 'now', 'once',\n",
    "        'when', 'because','in',\n",
    "        'why', 'what', 'which', 'who', \n",
    "         'how', 'where','just', 'both', \n",
    "        'with', 'then']\n",
    "\n",
    "conjunctions = ['for', 'and', 'nor', 'but', 'or', 'yet', 'so',\n",
    "        'though', 'although', 'even though', 'while', 'if', 'only if', 'unless',\n",
    "        'until', 'provided that', 'assuming that', 'even if', 'in case', 'than', 'rather than',\n",
    "        'whether', 'as much as', 'whereas', 'after', 'as long as', 'as soon as', 'before',\n",
    "        'by the time', 'now that', 'once', 'since', 'till', 'until',\n",
    "        'when', 'whenever', 'while', 'because', 'since', 'so that', 'in order',\n",
    "        'why', 'that', 'what', 'whatever', 'which', 'whichever', 'who', 'whoever',\n",
    "        'whom', 'whomever', 'whose', 'how', 'as though', 'as if','where', 'wherever',\n",
    "        'also', 'besides', 'furthermore', 'likewise', 'moreover', 'however', 'nevertheless',\n",
    "        'nonetheless', 'still', 'conversely', 'instead', 'otherwise', 'rather', 'accordingly',\n",
    "        'consequently', 'hence', 'meanwhile', 'then', 'therefore', 'thus']\n",
    "\n",
    "NEGATE = {'ain\\'t', 'aren\\'t', 'cannot', 'can\\'t', 'couldn\\'t', 'daren\\'t', 'didn\\'t', 'doesn\\'t',\n",
    " 'ain\\'t', 'aren\\'t', 'cant', 'couldn\\'t', 'daren\\'t', 'didn\\'t', 'doesn\\'t',\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", 'neither',\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",'!'}\n",
    "\n",
    "HAPPY = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P'\n",
    "    , ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "SAD = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFromStopWords(keep):\n",
    "    for i in range(len(keep)):\n",
    "        stop_words.remove(keep[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def lem_tokens(tokens, lmtzr):\n",
    "    lemmed = []\n",
    "    for item in tokens:\n",
    "        lemmed.append(lmtzr.lemmatize(item))\n",
    "    return lemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(sentencePart):\n",
    "\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    pos_word_list=[]\n",
    "    neu_word_list=[]\n",
    "    neg_word_list=[]\n",
    "    emoji=[]\n",
    "\n",
    "    for word in sentencePart:\n",
    "\n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.3 or word in HAPPY:\n",
    "            pos_word_list.append(word)\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.3 or word in NEGATE or word in SAD:\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)\n",
    "        if  emoji  or word in {':','=','>',';','X'}:\n",
    "            emoji.append(word)\n",
    "            if word in neu_word_list:\n",
    "                neu_word_list.remove(word)\n",
    "\n",
    "    emoji=''.join(emoji)\n",
    "    if emoji in HAPPY :\n",
    "        pos_word_list.append(emoji) \n",
    "    if emoji in SAD:\n",
    "        neg_word_list.append(emoji)\n",
    "    #print('Positive :',pos_word_list)        \n",
    "    #print('Neutral :',neu_word_list)    \n",
    "    #print('Negative :',neg_word_list)\n",
    "    vectormapping=[]\n",
    "    if len(sentencePart)!=0 :\n",
    "        probneg=len(neg_word_list)/len(sentencePart)\n",
    "        probpos=len(pos_word_list)/len(sentencePart)\n",
    "        probnue=len(neu_word_list)/len(sentencePart)\n",
    "    else: \n",
    "        probpos=probneg=probnue=0\n",
    "    #print(vectormapping)\n",
    "    return probpos,probneg,probnue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', \"it's\", 'should', 'them', 'will', \"didn't\", 'can', \"'s\", \"you'll\", ',', 'most', 'they', 'were', 'down', 'out', \"hadn't\", \"wouldn't\", 'am', 'during', 'his', 'whom', 'over', 'don', \"wasn't\", '.', 'been', \"she's\", 'own', 'd', 'was', \"shan't\", 'those', 'theirs', 'here', 'her', 'herself', \"shouldn't\", 'wouldn', 'isn', 'off', 'few', 'have', 'themselves', \"needn't\", 'through', 'our', 'their', 'these', 'is', 'ourselves', 'yours', 'shouldn', 'an', 'between', 'wasn', 't', 'has', 'haven', 'further', 'did', 'she', 'do', 'are', 'shan', 'had', \"mustn't\", 'ma', 'doesn', \"you'd\", 'it', 'the', 'each', 'himself', \"weren't\", 'too', \"aren't\", 'hasn', 'does', 'doing', 'to', 'itself', 'below', 'yourself', 'against', 'your', \"doesn't\", 'be', \"should've\", '...', 'ours', 'that', 'on', 'hers', 'such', 'into', \"don't\", '..', 've', 'i', 'very', 'didn', 'll', 'up', 's', 'couldn', 'm', \"hasn't\", 'we', 'my', \"that'll\", 'me', 'yourselves', 'mustn', 'from', \"isn't\", 'all', 'he', 'o', 'hadn', 'other', 'same', \"mightn't\", 'its', 'having', 'at', 'of', 'being', 're', 'more', 'won', 'you', 'ain', 'any', 'weren', 'y', 'mightn', 'this', 'a', \"you've\", 'aren', 'again', 'there', 'about', \"won't\", 'under', 'needn', \"couldn't\", 'myself', 'above', 'some', \"you're\", \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add(',')\n",
    "stop_words.add('.')\n",
    "stop_words.add('...')\n",
    "stop_words.add('..')\n",
    "stop_words.add(\"'s\")\n",
    "\n",
    "removeFromStopWords(keep)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalironic=0;\n",
    "ironicaccuracy=0;\n",
    "SEMANTICVECTOR=[]\n",
    "with open(\"SemEval2018-T3-train-taskA.txt\", encoding=\"utf8\") as ins:\n",
    "    tweets = []\n",
    "    for line in ins:\n",
    "        new = re.split(r'\\t+', line.rstrip('\\t'))\n",
    "        newString =new[2].rstrip('\\n')\n",
    "        newString = re.sub(r\"http\\S+\", \"\", newString)\n",
    "        newString = re.sub(r\"@\\S+ \", \"\", newString)\n",
    "        newString = re.sub(r\"#\", \"\", newString)\n",
    "        newString = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', newString)\n",
    "        newString = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', newString).lower()\n",
    "        firstString = newString\n",
    "        data = word_tokenize(firstString)\n",
    "        tweets.append(firstString)\n",
    "#model=wordtovec(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SemEval2018-T3-train-taskA.txt\", encoding=\"utf8\") as ins:\n",
    "    array = []\n",
    "    probablities=[]\n",
    "    for line in ins:\n",
    "        \n",
    "        new = re.split(r'\\t+', line.rstrip('\\t'))\n",
    "        index.append(new[0])\n",
    "        labels.append(new[1])\n",
    "        label=new[1]\n",
    "        newString =new[2].rstrip('\\n')\n",
    "        newString = re.sub(r\"http\\S+\", \"\", newString)\n",
    "        newString = re.sub(r\"@\\S+ \", \"\", newString)\n",
    "        newString = re.sub(r\"#\", \"\", newString)\n",
    "        newString = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', newString)\n",
    "        newString = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', newString).lower()\n",
    "        firstString = newString\n",
    "        array.append(firstString)\n",
    "        #print('firstString')\n",
    "        #print(firstString)\n",
    "        #split tweet into two if a conjuction exist\n",
    "        #for i in range(len(conjunctions)):\n",
    "        for i in range(len(conjunctions)):\n",
    "            if conjunctions[i] in firstString:\n",
    "                \n",
    "                #print(conjunctions[i])\n",
    "                split = firstString.split(conjunctions[i])\n",
    "                #print(split)\n",
    "\n",
    "                lastStringPart1 = word_tokenize(split[0])\n",
    "                lastStringPart2 = word_tokenize(split[1])\n",
    "                \n",
    "                filtered_sentence1 = [w for w in lastStringPart1 if not w in stop_words]\n",
    "                filtered_sentence2 = [w for w in lastStringPart2 if not w in stop_words]\n",
    "                \n",
    "                #print(filtered_sentence1)\n",
    "                #print(filtered_sentence2)\n",
    "                #print('the ironic counter',totalironic)\n",
    "                #print('the accuracy so far:',ironicaccuracy) \n",
    "                pos1,neg1,nue1=sentimentAnalysis(filtered_sentence1)\n",
    "                pos2,neg2,nue2=sentimentAnalysis(filtered_sentence2)\n",
    "                #wordtovec=wordtovec([filtered_sentence1,filtered_sentence2])\n",
    "                probablities.append([(pos1+pos2)/2,(neg1+neg2)/2,(nue1+nue2)/2])\n",
    "                \n",
    "                #print('the sentence has conjunction')\n",
    "                if  neg1>pos1 and pos2>neg2 or max(neg1,neg2)>0.5 or max(pos1,pos2)>0.5:\n",
    "                    totalironic+=1\n",
    "                 #print('the sentence classifed ironic with prob',neg1+pos2)\n",
    "                if label=='1':\n",
    "                    ironicaccuracy+=1 \n",
    "                   #print('the ironic counter accuracy',ironicaccuracy)\n",
    "                elif neg2>pos2 and pos1>neg1 or max(neg1,neg2)>0.5 or max(pos1,pos2)>0.5 :\n",
    "                    totalironic+=1\n",
    "                 #print('the sentence classifed ironic with prob',neg2+pos1) \n",
    "                if label=='1':\n",
    "                    ironicaccuracy+=1\n",
    "                   \n",
    "                else :\n",
    "                 #print('the sentence isnt ironic')\n",
    "                    if label=='0':\n",
    "                        ironicaccuracy+=1   \n",
    "                 \n",
    "                break\n",
    "            else: \n",
    "                #print('the ironic counter',totalironic)\n",
    "                lastString = word_tokenize(firstString)\n",
    "                filtered_sentence = [w for w in lastString if not w in stop_words]\n",
    "                pos,neg,nue=sentimentAnalysis(filtered_sentence)\n",
    "                #wordtovec=wordtovec(filtered_sentence)\n",
    "                probablities.append([pos,neg,nue])\n",
    "                if pos < neg or neg >0.5 or pos>0.5:\n",
    "                    totalironic+=1\n",
    "                 #print('the sentence classifed ironic with prob',neg) \n",
    "                if label=='1':\n",
    "                    ironicaccuracy+=1 \n",
    "                   \n",
    "                else :\n",
    "                 #print('the sentence isnt ironic')\n",
    "                    if label=='0':\n",
    "                        ironicaccuracy+=1 \n",
    "                break\n",
    "#print('ironicaccuracy') \n",
    "#print(ironicaccuracy)  \n",
    "#print(probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet index\n",
      "Label\n",
      "tweet text\n"
     ]
    }
   ],
   "source": [
    "print(index[0])\n",
    "print(labels[0])\n",
    "print(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of semantic analsysis\n",
      "the error of naive bayes classification\n",
      "56\n",
      "the accuracy of naive bayes classification\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manasvi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error of svm classification\n",
      "55\n",
      "the accuracy of svm  classification\n",
      "45\n",
      "the error of decision trees  classification\n",
      "43\n",
      "the accuracy of decision trees  classification\n",
      "57\n",
      "the error of nearest neighboor classification\n",
      "44\n",
      "the accuracy of nearest neighboor  classification\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "del index[0]\n",
    "index = list(map(int, index))\n",
    "del labels[0]\n",
    "labels = list(map(int, labels))\n",
    "del array[0]\n",
    "X = probablities[:3700]\n",
    "Y = labels[:3700]\n",
    "X1 = probablities[-100:]\n",
    "Y1 = labels[-100:]\n",
    "\n",
    "clf = GaussianNB()\n",
    "y_pred = clf.fit(X, Y).predict(X1)\n",
    "file = open(\"naive.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "    file.write(\"%f \" %item)\n",
    "    file.write(\"_\")\n",
    "file.close()\n",
    "\n",
    "print(\"results of semantic analsysis\")\n",
    "print('the error of naive bayes classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of naive bayes classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "\n",
    "model = SVC(gamma='scale')\n",
    "clf1 = SVC()\n",
    "y_pred = clf1.fit(X, Y).predict(X1)\n",
    "print('the error of svm classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of svm  classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "file = open(\"svm.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "    file.write(\"%f \" %item)\n",
    "    file.write(\"_\")\n",
    "file.close()\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred= clf.fit(X, Y).predict(X1)\n",
    "print('the error of decision trees  classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of decision trees  classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "file = open(\"trees.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "    file.write(\"%f  \" %item)\n",
    "    file.write(\"_\")\n",
    "file.close()\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1);\n",
    "model.fit(X, Y);\n",
    "Y_pred=model.predict(X1);\n",
    "accuracy=(Y1 ==Y_pred).sum()\n",
    "print('the error of nearest neighboor classification')\n",
    "print(100-accuracy)\n",
    "print('the accuracy of nearest neighboor  classification')\n",
    "print(accuracy)\n",
    "file = open(\"nearest.txt\",\"w\") \n",
    "for item in Y_pred:\n",
    "    file.write(\"%f\" %item)\n",
    "    file.write(\"_\")\n",
    "file.close()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
