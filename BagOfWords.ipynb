{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(tweets):\n",
    "    words = set(tweets)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def lem_tokens(tokens, lmtzr):\n",
    "    lemmed = []\n",
    "    for item in tokens:\n",
    "        lemmed.append(lmtzr.lemmatize(item))\n",
    "    return lemmed\n",
    "\n",
    "def join_array(array):\n",
    "    array2 = []\n",
    "    for word in array:\n",
    "        array2.append(' '.join(word))\n",
    "    return array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(array):\n",
    "    vectorizer = CountVectorizer()\n",
    "    xarray = []\n",
    "    X = vectorizer.fit_transform(array) \n",
    "    xarray = X.toarray()\n",
    "    #print(X)\n",
    "    #print(xarray)\n",
    "    y = [sum(row[i] for row in xarray) for i in range(len(xarray[0]))]\n",
    "    #print(y)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    return y, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "index = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(',')\n",
    "stop_words.add('.')\n",
    "stop_words.add('...')\n",
    "stop_words.add('..')\n",
    "stop_words.add(\"'s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SemEval2018-T3-train-taskA.txt\", encoding=\"utf8\") as ins:\n",
    "    array = []\n",
    "    for line in ins:\n",
    "        array.append(line)\n",
    "        new = re.split(r'\\t+', line.rstrip('\\t'))\n",
    "        index.append(new[0])\n",
    "        labels.append(new[1])\n",
    "        newString =new[2].rstrip('\\n')\n",
    "        newString = re.sub(r\"http\\S+\", \"\", newString)\n",
    "        newString = re.sub(r\"@\\S+ \", \"\", newString)\n",
    "        newString = re.sub(r\"#\", \"\", newString)\n",
    "        newString = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', newString)\n",
    "        newString = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', newString).lower()\n",
    "        firstString = newString\n",
    "        lastString = word_tokenize(firstString)\n",
    "        filtered_sentence = [w for w in lastString if not w in stop_words]\n",
    "        stems = stem_tokens(filtered_sentence, stemmer)\n",
    "        lems = lem_tokens(filtered_sentence, lmtzr)\n",
    "        tweets.append(lems)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet index\n",
      "Label\n",
      "['tweet', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(index[0])\n",
    "print(labels[0])\n",
    "print(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del index[0]\n",
    "index = list(map(int, index))\n",
    "del labels[0]\n",
    "labels = list(map(int, labels))\n",
    "del tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "['sweet', 'united', 'nation', 'video', 'time', 'christmas', 'imagine', 'religion']\n"
     ]
    }
   ],
   "source": [
    "print(index[0])\n",
    "print(labels[0])\n",
    "print(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ironicTweets = []\n",
    "new_ironicTweets = []\n",
    "index_i = []\n",
    "\n",
    "nonTweets = []\n",
    "new_nonTweets = []\n",
    "index_n = []\n",
    "\n",
    "i=0 \n",
    "\n",
    "for word in tweets:\n",
    "    i = i+1\n",
    "    if labels[i-1] == 1:\n",
    "        ironicTweets.append(word)\n",
    "        index_i.append(i)\n",
    "    else:\n",
    "        nonTweets.append(word)\n",
    "        index_n.append(i)\n",
    "\n",
    "\n",
    "new_ironicTweets = join_array(ironicTweets)\n",
    "new_nonTweets = join_array(nonTweets)\n",
    "new_tweets = join_array(tweets)\n",
    "#print(len(new_ironicTweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ironic hashmap ####\n",
    "\n",
    "y,x = bag_of_words(new_ironicTweets)\n",
    "#returns number of occurrences \"y\" and words referencing them \"x\"\n",
    "\n",
    "hashmap = dict(zip(x, y))\n",
    "#connects x and y in a hashmap (dictionary)\n",
    "sorted_hash = sorted(hashmap.items(), key=operator.itemgetter(1))\n",
    "word_features = [x[0] for x in sorted_hash[-250:]]\n",
    "\n",
    "#print(word_features)\n",
    "\n",
    "#sorting the hashmap for our own knowledge purposes\n",
    "\n",
    "#prints the most repeated word in ironic tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### non-Ironic hashmap ####\n",
    "\n",
    "y2,x2 = bag_of_words(new_nonTweets)\n",
    "\n",
    "hashmap2 = dict(zip(x2, y2))\n",
    "sorted_hash2 = sorted(hashmap2.items(), key=operator.itemgetter(1))\n",
    "word_features2 = [x[0] for x in sorted_hash2[-250:]]\n",
    "\n",
    "#prints the most repeated word in non-ironic tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### All-tweets hashmap ####\n",
    "\n",
    "y3,x3 = bag_of_words(new_tweets)\n",
    "\n",
    "hashmap3 = dict(zip(x3, y3))\n",
    "sorted_hash3 = sorted(hashmap3.items(), key=operator.itemgetter(1))\n",
    "word_features3 = [x[0] for x in sorted_hash3[-500:]]\n",
    "\n",
    "#prints the most repeated word in all tweets\n",
    "\n",
    "word_features = sum([word_features, word_features2], [])\n",
    "#print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    feature = []\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        #print(features[w],w)\n",
    "        if (w in words):\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    \n",
    "    \n",
    "\n",
    "    return (feature)\n",
    "\n",
    "feature = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    feature.append(find_features(tweet))\n",
    "    \n",
    "#print(len(feature[0]))\n",
    "#print(feature[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of bag of words\n",
      "the error of naive bayes classification\n",
      "36\n",
      "the accuracy of naive bayes  classification\n",
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manasvi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error of svm classification\n",
      "38\n",
      "the accuracy of svm  classification\n",
      "62\n",
      "the error of trees classification\n",
      "41\n",
      "the accuracy of trees classification\n",
      "59\n",
      "the error of nearest neighboor classification\n",
      "49\n",
      "the accuracy of nearest neighboor  classification\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "X = feature[:3700]\n",
    "Y = labels[:3700]\n",
    "X1 = feature[-100:]\n",
    "Y1 = labels[-100:]\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "y_pred = clf.fit(X, Y).predict(X1)\n",
    "\n",
    "print(\"results of bag of words\")\n",
    "\n",
    "print('the error of naive bayes classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of naive bayes  classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "file = open(\"naive-bag.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "  file.write(\"%f\" %item)\n",
    "  file.write(\"_\")\n",
    "file.close() \n",
    "clf1 = svm.SVC()\n",
    "y_pred = clf1.fit(X, Y).predict(X1)\n",
    "print('the error of svm classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of svm  classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "file = open(\"svm-bag.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "  file.write(\"%f\" %item)\n",
    "  file.write(\"_\")\n",
    "file.close() \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred= clf.fit(X, Y).predict(X1)\n",
    "print('the error of trees classification')\n",
    "print((Y1 != y_pred).sum())\n",
    "print('the accuracy of trees classification')\n",
    "print((Y1 == y_pred).sum())\n",
    "file = open(\"trees-bag.txt\",\"w\") \n",
    "for item in y_pred:\n",
    "  file.write(\"%f\" %item)\n",
    "  file.write(\"_\")\n",
    "file.close()\n",
    "model = KNeighborsClassifier(n_neighbors=1);\n",
    "model.fit(X, Y);\n",
    "Y_pred=model.predict(X1);\n",
    "accuracy=(Y1 ==Y_pred).sum()\n",
    "print('the error of nearest neighboor classification')\n",
    "print(100-accuracy)\n",
    "print('the accuracy of nearest neighboor  classification')\n",
    "print(accuracy)\n",
    "file = open(\"nearest-bag.txt\",\"w\") \n",
    "for item in Y_pred:\n",
    "  file.write(\"%f\" %item)\n",
    "  file.write(\"_\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = []\n",
    "tweet_ex = \"I am dancing.\"\n",
    "plain_tweet = tweet_ex\n",
    "plain_tweet = re.sub(r\"http\\S+\", \"\", plain_tweet)\n",
    "plain_tweet = re.sub(r\"@\\S+ \", \"\", plain_tweet)\n",
    "plain_tweet = re.sub(r\"#\", \"\", plain_tweet)\n",
    "plain_tweet = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', plain_tweet)\n",
    "plain_tweet = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', plain_tweet).lower()\n",
    "tokenized_tweet = word_tokenize(plain_tweet)\n",
    "filtered_tweet = [w for w in tokenized_tweet if not w in stop_words]\n",
    "stems = stem_tokens(filtered_tweet, stemmer)\n",
    "lems = lem_tokens(filtered_tweet, lmtzr)\n",
    "tweet_tokens.append(lems)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ironic tweet :1 ; non-ironic tweet: 0 \n",
    "for x in tweet_tokens:\n",
    "    features_ex = find_features(x)\n",
    "\n",
    "list_1 = [[]]\n",
    "list_1[0] = features_ex\n",
    "model = GaussianNB()\n",
    "model.fit(X,Y)\n",
    "predicted_y = model.predict(list_1)\n",
    "print(predicted_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
